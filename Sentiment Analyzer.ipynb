{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'CONGRATULATIONS!!', 'external_id': None, 'error': False, 'classifications': [{'tag_name': 'Positive', 'tag_id': 122921383, 'confidence': 0.948}]}]\n"
     ]
    }
   ],
   "source": [
    "# from monkeylearn import MonkeyLearn\n",
    "\n",
    "# ml = MonkeyLearn('20c05e15622fd83eaa611124c2fb313f030e5928')\n",
    "# data = [\"CONGRATULATIONS!!\"]\n",
    "# model_id = 'cl_pi3C7JiL'\n",
    "# result = ml.classifiers.classify(model_id, data)\n",
    "# print(result.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Love\n",
      "[nltk_data]     Karnval\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think we are making great progress on the systems side.  I would like to\n",
      "set a deadline of November 10th to have a plan on all North American projects\n",
      "(I'm ok if fundementals groups are excluded) that is signed off on by\n",
      "commercial, Sally's world, and Beth's world.  When I say signed off I mean\n",
      "that I want signitures on a piece of paper that everyone is onside with the\n",
      "plan for each project.  If you don't agree don't sign. If certain projects\n",
      "(ie. the gas plan) are not done yet then lay out a timeframe that the plan\n",
      "will be complete.  I want much more in the way of specifics about objectives\n",
      "and timeframe. Thanks for everyone's hard work on this.\n",
      "compound: 0.8951, neg: 0.042, neu: 0.821, pos: 0.136, "
     ]
    }
   ],
   "source": [
    "# first, we import the relevant modules from the NLTK library\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# next, we initialize VADER so we can use it within our Python script\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# the variable 'message_text' now contains the text we will analyze.\n",
    "message_text = '''I think we are making great progress on the systems side.  I would like to\n",
    "set a deadline of November 10th to have a plan on all North American projects\n",
    "(I'm ok if fundementals groups are excluded) that is signed off on by\n",
    "commercial, Sally's world, and Beth's world.  When I say signed off I mean\n",
    "that I want signitures on a piece of paper that everyone is onside with the\n",
    "plan for each project.  If you don't agree don't sign. If certain projects\n",
    "(ie. the gas plan) are not done yet then lay out a timeframe that the plan\n",
    "will be complete.  I want much more in the way of specifics about objectives\n",
    "and timeframe. Thanks for everyone's hard work on this.'''\n",
    "\n",
    "print(message_text)\n",
    "\n",
    "# Calling the polarity_scores method on sid and passing in the message_text outputs a dictionary with negative, neutral, positive, and compound scores for the input text\n",
    "scores = sid.polarity_scores(message_text)\n",
    "\n",
    "# Here we loop through the keys contained in scores (pos, neu, neg, and compound scores) and print the key-value pairs on the screen\n",
    "\n",
    "for key in sorted(scores):\n",
    "        print('{0}: {1}, '.format(key, scores[key]), end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-cd8557ed007c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from google.colab import files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "train_tsv = files.upload()\n",
    "movies_train = pd.read_csv(io.BytesIO(train_tsv['train.tsv']),sep='\\t')\n",
    "movies_train.head()\n",
    "movies_train.Sentiment.value_counts()\n",
    "countSentiment=movies_train.groupby('Sentiment').count()\n",
    "plt.bar(countSentiment.index.values, countSentiment['Phrase'])\n",
    "plt.xlabel('Movie Sentiments from Review')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.show()\n",
    "\n",
    "tfidf=TfidfVectorizer()\n",
    "tfidf_Text= tfidf.fit_transform(movies_train['Phrase'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_Text, movies_train['Sentiment'], test_size=0.3, random_state=123)\n",
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= model.predict(X_test)\n",
    "print(\"Accuracy of MultinomialNB using TF-IDF:\",metrics.accuracy_score(y_test, predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Tokenized statements are :  ['Hi Learners hope you are all good.', 'In your project there is an important step which you need to understand, the process of tokenization.', 'What is Tokenization?', 'Tokenization is the first step in Text Analytics.', 'When we need to find grammatical errors in text or semantic errors or perform emotion and sentiment analysis of different textual comments and feedback we would need to break down huge amount of textual data into subsequently smaller chunks, till the textual context is identified for proper analysis and obtaining insight from it.', 'On a big picture, this is a step towards Text Mining.']\n",
      "The Tokenized Words are :  ['Hi', 'Learners', 'hope', 'you', 'are', 'all', 'good', '.', 'In', 'your', 'project', 'there', 'is', 'an', 'important', 'step', 'which', 'you', 'need', 'to', 'understand', ',', 'the', 'process', 'of', 'tokenization', '.', 'What', 'is', 'Tokenization', '?', 'Tokenization', 'is', 'the', 'first', 'step', 'in', 'Text', 'Analytics', '.', 'When', 'we', 'need', 'to', 'find', 'grammatical', 'errors', 'in', 'text', 'or', 'semantic', 'errors', 'or', 'perform', 'emotion', 'and', 'sentiment', 'analysis', 'of', 'different', 'textual', 'comments', 'and', 'feedback', 'we', 'would', 'need', 'to', 'break', 'down', 'huge', 'amount', 'of', 'textual', 'data', 'into', 'subsequently', 'smaller', 'chunks', ',', 'till', 'the', 'textual', 'context', 'is', 'identified', 'for', 'proper', 'analysis', 'and', 'obtaining', 'insight', 'from', 'it', '.', 'On', 'a', 'big', 'picture', ',', 'this', 'is', 'a', 'step', 'towards', 'Text', 'Mining', '.']\n",
      "Sampling  <FreqDist with 75 samples and 108 outcomes>\n",
      "The first 3 frequently used tokens are\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('.', 5), ('is', 5), ('step', 3)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "text=\"\"\"Hi Learners hope you are all good. In your project there is an important step which you need to understand, the process of tokenization. What is Tokenization? Tokenization is the first step in Text Analytics. When we need to find grammatical errors in text or semantic errors or perform emotion and sentiment analysis of different textual comments and feedback we would need to break down huge amount of textual data into subsequently smaller chunks, till the textual context is identified for proper analysis and obtaining insight from it. On a big picture, this is a step towards Text Mining. \"\"\"\n",
    "tokenized_text=sent_tokenize(text)\n",
    "print('The Tokenized statements are : ',tokenized_text)\n",
    "tokenized_word=word_tokenize(text)\n",
    "print('The Tokenized Words are : ', tokenized_word)\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(tokenized_word)\n",
    "print('Sampling ', fdist)\n",
    "print('The first 3 frequently used tokens are')\n",
    "fdist.most_common(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fastText'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ec4488a2a64a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfastText\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'punkt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fastText'"
     ]
    }
   ],
   "source": [
    "import fastText\n",
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import csv\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import itertools\n",
    "import emoji\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "#\n",
    "# DATA CLEANING\n",
    "#\n",
    "#####################################################################################\n",
    "\n",
    "# emoticons\n",
    "def load_dict_smileys():\n",
    "    \n",
    "    return {\n",
    "        \":‑)\":\"smiley\",\n",
    "        \":-]\":\"smiley\",\n",
    "        \":-3\":\"smiley\",\n",
    "        \":->\":\"smiley\",\n",
    "        \"8-)\":\"smiley\",\n",
    "        \":-}\":\"smiley\",\n",
    "        \":)\":\"smiley\",\n",
    "        \":]\":\"smiley\",\n",
    "        \":3\":\"smiley\",\n",
    "        \":>\":\"smiley\",\n",
    "        \"8)\":\"smiley\",\n",
    "        \":}\":\"smiley\",\n",
    "        \":o)\":\"smiley\",\n",
    "        \":c)\":\"smiley\",\n",
    "        \":^)\":\"smiley\",\n",
    "        \"=]\":\"smiley\",\n",
    "        \"=)\":\"smiley\",\n",
    "        \":-))\":\"smiley\",\n",
    "        \":‑D\":\"smiley\",\n",
    "        \"8‑D\":\"smiley\",\n",
    "        \"x‑D\":\"smiley\",\n",
    "        \"X‑D\":\"smiley\",\n",
    "        \":D\":\"smiley\",\n",
    "        \"8D\":\"smiley\",\n",
    "        \"xD\":\"smiley\",\n",
    "        \"XD\":\"smiley\",\n",
    "        \":‑(\":\"sad\",\n",
    "        \":‑c\":\"sad\",\n",
    "        \":‑<\":\"sad\",\n",
    "        \":‑[\":\"sad\",\n",
    "        \":(\":\"sad\",\n",
    "        \":c\":\"sad\",\n",
    "        \":<\":\"sad\",\n",
    "        \":[\":\"sad\",\n",
    "        \":-||\":\"sad\",\n",
    "        \">:[\":\"sad\",\n",
    "        \":{\":\"sad\",\n",
    "        \":@\":\"sad\",\n",
    "        \">:(\":\"sad\",\n",
    "        \":'‑(\":\"sad\",\n",
    "        \":'(\":\"sad\",\n",
    "        \":‑P\":\"playful\",\n",
    "        \"X‑P\":\"playful\",\n",
    "        \"x‑p\":\"playful\",\n",
    "        \":‑p\":\"playful\",\n",
    "        \":‑Þ\":\"playful\",\n",
    "        \":‑þ\":\"playful\",\n",
    "        \":‑b\":\"playful\",\n",
    "        \":P\":\"playful\",\n",
    "        \"XP\":\"playful\",\n",
    "        \"xp\":\"playful\",\n",
    "        \":p\":\"playful\",\n",
    "        \":Þ\":\"playful\",\n",
    "        \":þ\":\"playful\",\n",
    "        \":b\":\"playful\",\n",
    "        \"<3\":\"love\"\n",
    "        }\n",
    "\n",
    "# self defined contractions\n",
    "def load_dict_contractions():\n",
    "    \n",
    "    return {\n",
    "        \"ain't\":\"is not\",\n",
    "        \"amn't\":\"am not\",\n",
    "        \"aren't\":\"are not\",\n",
    "        \"can't\":\"cannot\",\n",
    "        \"'cause\":\"because\",\n",
    "        \"couldn't\":\"could not\",\n",
    "        \"couldn't've\":\"could not have\",\n",
    "        \"could've\":\"could have\",\n",
    "        \"daren't\":\"dare not\",\n",
    "        \"daresn't\":\"dare not\",\n",
    "        \"dasn't\":\"dare not\",\n",
    "        \"didn't\":\"did not\",\n",
    "        \"doesn't\":\"does not\",\n",
    "        \"don't\":\"do not\",\n",
    "        \"e'er\":\"ever\",\n",
    "        \"em\":\"them\",\n",
    "        \"everyone's\":\"everyone is\",\n",
    "        \"finna\":\"fixing to\",\n",
    "        \"gimme\":\"give me\",\n",
    "        \"gonna\":\"going to\",\n",
    "        \"gon't\":\"go not\",\n",
    "        \"gotta\":\"got to\",\n",
    "        \"hadn't\":\"had not\",\n",
    "        \"hasn't\":\"has not\",\n",
    "        \"haven't\":\"have not\",\n",
    "        \"he'd\":\"he would\",\n",
    "        \"he'll\":\"he will\",\n",
    "        \"he's\":\"he is\",\n",
    "        \"he've\":\"he have\",\n",
    "        \"how'd\":\"how would\",\n",
    "        \"how'll\":\"how will\",\n",
    "        \"how're\":\"how are\",\n",
    "        \"how's\":\"how is\",\n",
    "        \"I'd\":\"I would\",\n",
    "        \"I'll\":\"I will\",\n",
    "        \"I'm\":\"I am\",\n",
    "        \"I'm'a\":\"I am about to\",\n",
    "        \"I'm'o\":\"I am going to\",\n",
    "        \"isn't\":\"is not\",\n",
    "        \"it'd\":\"it would\",\n",
    "        \"it'll\":\"it will\",\n",
    "        \"it's\":\"it is\",\n",
    "        \"I've\":\"I have\",\n",
    "        \"kinda\":\"kind of\",\n",
    "        \"let's\":\"let us\",\n",
    "        \"mayn't\":\"may not\",\n",
    "        \"may've\":\"may have\",\n",
    "        \"mightn't\":\"might not\",\n",
    "        \"might've\":\"might have\",\n",
    "        \"mustn't\":\"must not\",\n",
    "        \"mustn't've\":\"must not have\",\n",
    "        \"must've\":\"must have\",\n",
    "        \"needn't\":\"need not\",\n",
    "        \"ne'er\":\"never\",\n",
    "        \"o'\":\"of\",\n",
    "        \"o'er\":\"over\",\n",
    "        \"ol'\":\"old\",\n",
    "        \"oughtn't\":\"ought not\",\n",
    "        \"shalln't\":\"shall not\",\n",
    "        \"shan't\":\"shall not\",\n",
    "        \"she'd\":\"she would\",\n",
    "        \"she'll\":\"she will\",\n",
    "        \"she's\":\"she is\",\n",
    "        \"shouldn't\":\"should not\",\n",
    "        \"shouldn't've\":\"should not have\",\n",
    "        \"should've\":\"should have\",\n",
    "        \"somebody's\":\"somebody is\",\n",
    "        \"someone's\":\"someone is\",\n",
    "        \"something's\":\"something is\",\n",
    "        \"that'd\":\"that would\",\n",
    "        \"that'll\":\"that will\",\n",
    "        \"that're\":\"that are\",\n",
    "        \"that's\":\"that is\",\n",
    "        \"there'd\":\"there would\",\n",
    "        \"there'll\":\"there will\",\n",
    "        \"there're\":\"there are\",\n",
    "        \"there's\":\"there is\",\n",
    "        \"these're\":\"these are\",\n",
    "        \"they'd\":\"they would\",\n",
    "        \"they'll\":\"they will\",\n",
    "        \"they're\":\"they are\",\n",
    "        \"they've\":\"they have\",\n",
    "        \"this's\":\"this is\",\n",
    "        \"those're\":\"those are\",\n",
    "        \"'tis\":\"it is\",\n",
    "        \"'twas\":\"it was\",\n",
    "        \"wanna\":\"want to\",\n",
    "        \"wasn't\":\"was not\",\n",
    "        \"we'd\":\"we would\",\n",
    "        \"we'd've\":\"we would have\",\n",
    "        \"we'll\":\"we will\",\n",
    "        \"we're\":\"we are\",\n",
    "        \"weren't\":\"were not\",\n",
    "        \"we've\":\"we have\",\n",
    "        \"what'd\":\"what did\",\n",
    "        \"what'll\":\"what will\",\n",
    "        \"what're\":\"what are\",\n",
    "        \"what's\":\"what is\",\n",
    "        \"what've\":\"what have\",\n",
    "        \"when's\":\"when is\",\n",
    "        \"where'd\":\"where did\",\n",
    "        \"where're\":\"where are\",\n",
    "        \"where's\":\"where is\",\n",
    "        \"where've\":\"where have\",\n",
    "        \"which's\":\"which is\",\n",
    "        \"who'd\":\"who would\",\n",
    "        \"who'd've\":\"who would have\",\n",
    "        \"who'll\":\"who will\",\n",
    "        \"who're\":\"who are\",\n",
    "        \"who's\":\"who is\",\n",
    "        \"who've\":\"who have\",\n",
    "        \"why'd\":\"why did\",\n",
    "        \"why're\":\"why are\",\n",
    "        \"why's\":\"why is\",\n",
    "        \"won't\":\"will not\",\n",
    "        \"wouldn't\":\"would not\",\n",
    "        \"would've\":\"would have\",\n",
    "        \"y'all\":\"you all\",\n",
    "        \"you'd\":\"you would\",\n",
    "        \"you'll\":\"you will\",\n",
    "        \"you're\":\"you are\",\n",
    "        \"you've\":\"you have\",\n",
    "        \"Whatcha\":\"What are you\",\n",
    "        \"luv\":\"love\",\n",
    "        \"sux\":\"sucks\"\n",
    "        }\n",
    "\n",
    "\n",
    "def tweet_cleaning_for_sentiment_analysis(tweet):    \n",
    "    \n",
    "    #Escaping HTML characters\n",
    "    tweet = BeautifulSoup(tweet).get_text()\n",
    "   \n",
    "    #Special case not handled previously.\n",
    "    tweet = tweet.replace('\\x92',\"'\")\n",
    "    \n",
    "    #Removal of hastags/account\n",
    "    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", \" \", tweet).split())\n",
    "    \n",
    "    #Removal of address\n",
    "    tweet = ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "    \n",
    "    #Removal of Punctuation\n",
    "    tweet = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=]\", \" \", tweet).split())\n",
    "    \n",
    "    #Lower case\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    #CONTRACTIONS source: https://en.wikipedia.org/wiki/Contraction_%28grammar%29\n",
    "    CONTRACTIONS = load_dict_contractions()\n",
    "    tweet = tweet.replace(\"’\",\"'\")\n",
    "    words = tweet.split()\n",
    "    reformed = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in words]\n",
    "    tweet = \" \".join(reformed)\n",
    "    \n",
    "    # Standardizing words\n",
    "    tweet = ''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet))\n",
    "    \n",
    "    #Deal with emoticons source: https://en.wikipedia.org/wiki/List_of_emoticons\n",
    "    SMILEY = load_dict_smileys()  \n",
    "    words = tweet.split()\n",
    "    reformed = [SMILEY[word] if word in SMILEY else word for word in words]\n",
    "    tweet = \" \".join(reformed)\n",
    "    \n",
    "    #Deal with emojis\n",
    "    tweet = emoji.demojize(tweet)\n",
    "\n",
    "    tweet = tweet.replace(\":\",\" \")\n",
    "    tweet = ' '.join(tweet.split())\n",
    "\n",
    "    return tweet\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "#\n",
    "# DATA PROCESSING\n",
    "#\n",
    "#####################################################################################\n",
    "\n",
    "def transform_instance(row):\n",
    "    cur_row = []\n",
    "    #Prefix the index-ed label with __label__\n",
    "    label = \"__label__\" + row[4]  \n",
    "    cur_row.append(label)\n",
    "    cur_row.extend(nltk.word_tokenize(tweet_cleaning_for_sentiment_analysis(row[2].lower())))\n",
    "    return cur_row\n",
    "\n",
    "\n",
    "def preprocess(input_file, output_file, keep=1):\n",
    "    i=0\n",
    "    with open(output_file, 'w') as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "        with open(input_file, 'r', newline='', encoding='latin1') as csvinfile: #,encoding='latin1'\n",
    "            csv_reader = csv.reader(csvinfile, delimiter=',', quotechar='\"')\n",
    "            for row in csv_reader:\n",
    "                if row[4]!=\"MIXED\" and row[4].upper() in ['POSITIVE','NEGATIVE','NEUTRAL'] and row[2]!='':\n",
    "                    row_output = transform_instance(row)\n",
    "                    csv_writer.writerow(row_output )\n",
    "                    # print(row_output)\n",
    "                i=i+1\n",
    "                if i%10000 ==0:\n",
    "                    print(i)\n",
    "            \n",
    "# Preparing the training dataset        \n",
    "preprocess('betsentiment-EN-tweets-sentiment-teams.csv', 'tweets.train')\n",
    "\n",
    "# Preparing the validation dataset        \n",
    "preprocess('betsentiment-EN-tweets-sentiment-players.csv', 'tweets.validation')\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "#\n",
    "# UPSAMPLING\n",
    "#\n",
    "#####################################################################################\n",
    "\n",
    "def upsampling(input_file, output_file, ratio_upsampling=1):\n",
    "    # Create a file with equal number of tweets for each label\n",
    "    #    input_file: path to file\n",
    "    #    output_file: path to the output file\n",
    "    #    ratio_upsampling: ratio of each minority classes vs majority one. 1 mean there will be as much of each class than there is for the majority class \n",
    "    \n",
    "    i=0\n",
    "    counts = {}\n",
    "    dict_data_by_label = {}\n",
    "\n",
    "    # GET LABEL LIST AND GET DATA PER LABEL\n",
    "    with open(input_file, 'r', newline='') as csvinfile: \n",
    "        csv_reader = csv.reader(csvinfile, delimiter=',', quotechar='\"')\n",
    "        for row in csv_reader:\n",
    "            counts[row[0].split()[0]] = counts.get(row[0].split()[0], 0) + 1\n",
    "            if not row[0].split()[0] in dict_data_by_label:\n",
    "                dict_data_by_label[row[0].split()[0]]=[row[0]]\n",
    "            else:\n",
    "                dict_data_by_label[row[0].split()[0]].append(row[0])\n",
    "            i=i+1\n",
    "            if i%10000 ==0:\n",
    "                print(\"read\" + str(i))\n",
    "\n",
    "    # FIND MAJORITY CLASS\n",
    "    majority_class=\"\"\n",
    "    count_majority_class=0\n",
    "    for item in dict_data_by_label:\n",
    "        if len(dict_data_by_label[item])>count_majority_class:\n",
    "            majority_class= item\n",
    "            count_majority_class=len(dict_data_by_label[item])  \n",
    "    \n",
    "    # UPSAMPLE MINORITY CLASS\n",
    "    data_upsampled=[]\n",
    "    for item in dict_data_by_label:\n",
    "        data_upsampled.extend(dict_data_by_label[item])\n",
    "        if item != majority_class:\n",
    "            items_added=0\n",
    "            items_to_add = count_majority_class - len(dict_data_by_label[item])\n",
    "            while items_added<items_to_add:\n",
    "                data_upsampled.extend(dict_data_by_label[item][:max(0,min(items_to_add-items_added,len(dict_data_by_label[item])))])\n",
    "                items_added = items_added + max(0,min(items_to_add-items_added,len(dict_data_by_label[item])))\n",
    "\n",
    "    # WRITE ALL\n",
    "    i=0\n",
    "\n",
    "    with open(output_file, 'w') as txtoutfile:\n",
    "        for row in data_upsampled:\n",
    "            txtoutfile.write(row+ '\\n' )\n",
    "            i=i+1\n",
    "            if i%10000 ==0:\n",
    "                print(\"writer\" + str(i))\n",
    "\n",
    "\n",
    "upsampling( 'tweets.train','uptweets.train')\n",
    "# No need to upsample for the validation set. As it does not matter what validation set contains.\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "#\n",
    "# TRAINING\n",
    "#\n",
    "#####################################################################################\n",
    "\n",
    "# Full path to training data.\n",
    "training_data_path ='uptweets.train' \n",
    "validation_data_path ='tweets.validation'\n",
    "model_path =''\n",
    "model_name=\"model-en\"\n",
    "\n",
    "def train():\n",
    "    print('Training start')\n",
    "    try:\n",
    "        hyper_params = {\"lr\": 0.01,\n",
    "                        \"epoch\": 20,\n",
    "                        \"wordNgrams\": 2,\n",
    "                        \"dim\": 20}     \n",
    "                               \n",
    "        print(str(datetime.datetime.now()) + ' START=>' + str(hyper_params) )\n",
    "\n",
    "        # Train the model.\n",
    "        model = fastText.train_supervised(input=training_data_path, **hyper_params)\n",
    "        print(\"Model trained with the hyperparameter \\n {}\".format(hyper_params))\n",
    "\n",
    "        # CHECK PERFORMANCE\n",
    "        print(str(datetime.datetime.now()) + 'Training complete.' + str(hyper_params) )\n",
    "        \n",
    "        model_acc_training_set = model.test(training_data_path)\n",
    "        model_acc_validation_set = model.test(validation_data_path)\n",
    "        \n",
    "        # DISPLAY ACCURACY OF TRAINED MODEL\n",
    "        text_line = str(hyper_params) + \",accuracy:\" + str(model_acc_training_set[1])  + \", validation:\" + str(model_acc_validation_set[1]) + '\\n' \n",
    "        print(text_line)\n",
    "        \n",
    "        #quantize a model to reduce the memory usage\n",
    "        model.quantize(input=training_data_path, qnorm=True, retrain=True, cutoff=100000)\n",
    "        \n",
    "        print(\"Model is quantized!!\")\n",
    "        model.save_model(os.path.join(model_path,model_name + \".ftz\"))                \n",
    "    \n",
    "        ##########################################################################\n",
    "        #\n",
    "        #  TESTING PART\n",
    "        #\n",
    "        ##########################################################################            \n",
    "        model.predict(['why not'],k=3)\n",
    "        model.predict(['this player is so bad'],k=1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print('Exception during training: ' + str(e) )\n",
    "\n",
    "\n",
    "# Train your model.\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
